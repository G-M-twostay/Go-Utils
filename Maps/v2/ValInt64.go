// Code generated by go generate; DO NOT EDIT.
// Generated specializations of ValVal maps that exhausts atomicXXX functions based on ValUintptr.go and ValUintptr_test.go.
package v2

import (
	"github.com/g-m-twostay/go-utils/Maps/internal"
	"math/bits"
	"sync/atomic"
	"unsafe"
)

type ValInt64[K comparable, V ~int64] struct {
	base[K]
}

func NewValInt64[K comparable, V ~int64](minBucketSize, maxBucketSize byte, maxHash uint, hashF func(K) uint) *ValInt64[K, V] {
	vp := ValInt64[K, V]{
		base[K]{minAvgBucketSize: minBucketSize,
			maxAvgBucketSize: maxBucketSize,
			maxLogChunkSize:  byte(bits.Len(maxHash)),
			HashF:            hashF},
	}
	vp.buckets = newChunkArr(vp.maxLogChunkSize, vp.maxLogChunkSize)
	vp.buckets.set(0, &vp.firstRelay)
	return &vp
}

func (vv *ValInt64[K, V]) LoadAndDelete(key K) (V, bool) {
	hash := vv.HashF(key)
	for cur, curAddr := (*chunkArr)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&vv.buckets)))).Get(hash).walk(), unsafe.Pointer(nil); ; cur = (*relay)(curAddr).walk() {
		if curAddr = addr(cur); cur == nil || hash < (*relay)(curAddr).hash {
			return 0, false
		} else if (*relay)(curAddr).hash == hash && !isRelay(cur) && (*valNode[K, int64])(curAddr).key == key {
			if (*relay)(curAddr).mark() {
				vv.size.Add(^uintptr(resizingMask<<1 - 1))
				vv.tryMerge()
				return V(atomic.LoadInt64(&(*valNode[K, int64])(curAddr).val)), true
			}
			return 0, false
		}
	}
}
func (vv *ValInt64[K, V]) Load(key K) (V, bool) {
	hash := vv.HashF(key)
	for cur, curAddr := (*chunkArr)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&vv.buckets)))).Get(hash).walk(), unsafe.Pointer(nil); ; cur = (*relay)(curAddr).walk() {
		if curAddr = addr(cur); cur == nil || hash < (*relay)(curAddr).hash {
			return 0, false
		} else if (*relay)(curAddr).hash == hash && !isRelay(cur) && (*valNode[K, int64])(curAddr).key == key {
			return V(atomic.LoadInt64(&(*valNode[K, int64])(curAddr).val)), true
		}
	}
}
func (vv *ValInt64[K, V]) LoadPtr(key K) *V {
	hash := vv.HashF(key)
	for cur, curAddr := (*chunkArr)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&vv.buckets)))).Get(hash).walk(), unsafe.Pointer(nil); ; cur = (*relay)(curAddr).walk() {
		if curAddr = addr(cur); cur == nil || hash < (*relay)(curAddr).hash {
			return nil
		} else if (*relay)(curAddr).hash == hash && !isRelay(cur) && (*valNode[K, int64])(curAddr).key == key {
			return (*V)(unsafe.Pointer(&(*valNode[K, int64])(curAddr).val))
		}
	}
}
func (vv *ValInt64[K, V]) Store(key K, val V) bool {
	hash := vv.HashF(key)
	var new *valNode[K, int64]
	fb, path := func() *relay {
		return (*chunkArr)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&vv.buckets)))).Get(hash)
	}, internal.EvictStack{}
	for left, right := (*chunkArr)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&vv.buckets)))).Get(hash).crawl(&path, fb); ; left, right = left.crawl(&path, fb) {
		if rightAddr := addr(right); right == nil || hash < (*relay)(rightAddr).hash {
			if new == nil {
				new = &valNode[K, int64]{relay{hash: hash}, key, int64 (val)}
			}
			if new.next = right; left.tryLink(right, unsafe.Pointer(new)) {
				vv.size.Add(resizingMask << 1)
				vv.trySplit()
				return true
			}
		} else if (*relay)(rightAddr).hash == hash && !isRelay(right) && (*valNode[K, int64])(rightAddr).key == key {
			atomic.StoreInt64(&(*valNode[K, int64])(rightAddr).val, int64 (val))
			return false
		} else {
			path.Push(rightAddr)
			left = (*relay)(rightAddr)
		}
	}
}
func (vv *ValInt64[K, V]) LoadOrStore(key K, val V) (V, bool) {
	hash := vv.HashF(key)
	var new *valNode[K, int64]
	fb, path := func() *relay {
		return (*chunkArr)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&vv.buckets)))).Get(hash)
	}, internal.EvictStack{}
	for left, right := (*chunkArr)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&vv.buckets)))).Get(hash).crawl(&path, fb); ; left, right = left.crawl(&path, fb) {
		if rightAddr := addr(right); right == nil || hash < (*relay)(rightAddr).hash {
			if new == nil {
				new = &valNode[K, int64]{relay{hash: hash}, key, int64 (val)}
			}
			if new.next = right; left.tryLink(right, unsafe.Pointer(new)) {
				vv.size.Add(resizingMask << 1)
				vv.trySplit()
				return 0, false
			}
		} else if (*relay)(rightAddr).hash == hash && !isRelay(right) && (*valNode[K, int64])(rightAddr).key == key {
			return V(atomic.LoadInt64(&(*valNode[K, int64])(rightAddr).val)), true
		} else {
			path.Push(rightAddr)
			left = (*relay)(rightAddr)
		}
	}
}
func (vv *ValInt64[K, V]) Swap(key K, val V) (V, bool) {
	hash := vv.HashF(key)
	for cur, curAddr := (*chunkArr)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&vv.buckets)))).Get(hash).walk(), unsafe.Pointer(nil); ; cur = (*relay)(curAddr).walk() {
		if curAddr = addr(cur); cur == nil || hash < (*relay)(curAddr).hash {
			return 0, false
		} else if (*relay)(curAddr).hash == hash && !isRelay(cur) && (*valNode[K, int64])(curAddr).key == key {
			return V(atomic.SwapInt64(&(*valNode[K, int64])(curAddr).val, int64 (val))), true
		}
	}
}
func (vv *ValInt64[K, V]) CompareAndSwap(key K, old, new V) CASResult {
	hash := vv.HashF(key)
	for cur, curAddr := (*chunkArr)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&vv.buckets)))).Get(hash).walk(), unsafe.Pointer(nil); ; cur = (*relay)(curAddr).walk() {
		if curAddr = addr(cur); cur == nil || hash < (*relay)(curAddr).hash {
			return NULL
		} else if (*relay)(curAddr).hash == hash && !isRelay(cur) && (*valNode[K, int64])(curAddr).key == key {
			a := atomic.CompareAndSwapInt64(&(*valNode[K, int64])(curAddr).val, int64 (old), int64 (new))
			return *(*CASResult)(unsafe.Pointer(&a))
		}
	}
}

func (vv *ValInt64[K, V]) Take() (*K, V) {
	cur := vv.firstRelay.walk()
	for ; isRelay(cur); cur = (*relay)(addr(cur)).walk() {
	}
	if cur == nil {
		return nil, 0
	}
	a := (*valNode[K, int64])(cur)
	return &a.key, V(atomic.LoadInt64(&a.val))
}
func (vv *ValInt64[K, V]) Range(yield func(K, V) bool) {
	for cur, curAddr := vv.firstRelay.walk(), (unsafe.Pointer)(nil); cur != nil; cur = (*relay)(curAddr).walk() {
		if curAddr = addr(cur); !isRelay(cur) {
			if a := (*valNode[K, int64])(curAddr); !yield(a.key, V(atomic.LoadInt64(&a.val))) {
				break
			}
		}
	}
}
func (vv *ValInt64[K, V]) Copy() *ValInt64[K, V] {
	copied := ValInt64[K, V]{base[K]{minAvgBucketSize: vv.minAvgBucketSize, maxAvgBucketSize: vv.maxAvgBucketSize, maxLogChunkSize: vv.maxLogChunkSize, buckets: newChunkArr(vv.maxLogChunkSize, (*chunkArr)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&vv.buckets)))).logChunkSize), HashF: vv.HashF}}
	tail := &copied.firstRelay
	tailIndex := uint(0)
	copied.buckets.set(tailIndex, tail)
	for cur, curAddr := vv.firstRelay.walk(), (unsafe.Pointer)(nil); cur != nil; cur = (*relay)(curAddr).walk() {
		if curAddr = addr(cur); !isRelay(cur) {
			a := (*valNode[K, int64])(curAddr)
			if index := copied.buckets.Index(a.hash); index != tailIndex {
				new := &relay{hash: index * (1 << copied.buckets.logChunkSize)}
				tail.next = unsafe.Pointer(uintptr(unsafe.Pointer(new)) | relayMask)
				tail = new
				copied.buckets.set(index, new)
				tailIndex = index
			}
			tail.next = unsafe.Pointer(&valNode[K, int64]{relay{hash: a.hash}, a.key, atomic.LoadInt64(&(*valNode[K, int64])(curAddr).val)})
			tail = (*relay)(tail.next)
			copied.size.Add(resizingMask << 1)
		}
	}
	return &copied
}
